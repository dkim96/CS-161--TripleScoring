README.txt: instructions for downloading our data

As explained in our report, we have extracted features for each person from Wikipedia.
Please download features.zip from the following link:
https://drive.google.com/file/d/10PxjgTb9EjzvGpIUMWmO8ET3cbJzLHTa/view?usp=sharing

The zip file contains ~760000 json files, each of which has json representation of the
features we have extracted. The json files are located on the top level of the zip file.
Please extract all the zip files to codes/preproc/features. Assuming that features.zip 
is located in the base directory of our submission, and that your cwd is the base
directory of our submission, you may do so with the following command:
	unzip features.zip -d ./codes/preproc/features

The data was generated by running the following commands:
# Scrape data in batches, by scraping the people found in each of the
# person.partial.???? files found in /codes/specFiles/personsSplit
# Each person.partial.???? was generated by taking 500 lines from the given persons file.
./codes/preproc/batchScrapeAll.sh 100
./codes/preproc/batchScrapeAll.sh 200
./codes/preproc/batchScrapeAll.sh 300
./codes/preproc/batchScrapeAll.sh 400
./codes/preproc/batchScrapeAll.sh 500
./codes/preproc/batchScrapeAll.sh 600
./codes/preproc/batchScrapeAll.sh 700
./codes/preproc/batchScrapeAll.sh 800

# Then unzip the resulting features???.zip files into a single directory called
# features. Do this manually.

# Then produce features.zip
zip -r -j features.zip features
